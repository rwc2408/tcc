{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (/home/rodrigochavoni/.local/lib/python3.10/site-packages/scipy/linalg/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_preprocess\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Phrases\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mphrases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Phraser\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgsdmm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MovieGroupProcess\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/matutils.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (/home/rodrigochavoni/.local/lib/python3.10/site-packages/scipy/linalg/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tweets_prefeito_vacina.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        O Brasil vai decidir neste ano se é tarado por...\n",
       "1        A propósito, doutor Augusto Aras: @MPF_PGR Vos...\n",
       "2        De novo, Bolsonaro espalha mentiras sobre as v...\n",
       "3        Gente. É óbvio que Bolsonaro tá vacinado. Óbvi...\n",
       "4        2) Johnson perguntou se Bolsonaro tinha tomado...\n",
       "                               ...                        \n",
       "17296    Folgamos em saber que Dona Olinda não virou ja...\n",
       "17297    E, como jacaré parado vira bolsa, bora lutar! ...\n",
       "17298    Acompanhei ao longo da manhã a CPI da Covid co...\n",
       "17299    Não seremos “cobaias” de um desgoverno. Não é ...\n",
       "17300    Minha mãe está feliz da vida pq ontem foi o “g...\n",
       "Name: text, Length: 17301, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m(gensim\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39msimple_preprocess(\u001b[38;5;28mstr\u001b[39m(sentence), deacc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m----> 5\u001b[0m tokens_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msent_to_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 3\u001b[0m, in \u001b[0;36msent_to_words\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_to_words\u001b[39m(sentences):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m(\u001b[43mgensim\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39msimple_preprocess(\u001b[38;5;28mstr\u001b[39m(sentence), deacc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "tokens_text = list(sent_to_words(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_grams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    bigrams_text = [bigram_mod[doc] for doc in texts]\n",
    "    trigrams_text =  [trigram_mod[bigram_mod[doc]] for doc in bigrams_text]\n",
    "    return trigrams_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis '}' does not match opening parenthesis '(' (3070624635.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[100], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    tokens_reviews = make_n_grams(tokens_text})\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis '}' does not match opening parenthesis '('\n"
     ]
    }
   ],
   "source": [
    "tokens_reviews = make_n_grams(tokens_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New column name: text_Normalized\n"
     ]
    }
   ],
   "source": [
    "# Normalização dos dados (remoção de stopwords, pontuação, etc)\n",
    "def normalize_text(text: str):\n",
    "    stopword_list = list(nltk.corpus.stopwords.words('portuguese')) + ['https', 'anos', 'hoje', 'brasil', 'dia', 'todos', 'mil', 'vamos', 'ainda', 'sobre', 'pra', 'milhões', 'vai']\n",
    "    return  \" \".join([word for word in word_tokenize(text.lower()) if word not in stopword_list and word.isalpha()])\n",
    "\n",
    "# Geração de coluna normalizada com base em outra coluna\n",
    "def generate_normalized_column(data: pd.DataFrame, from_column: str):\n",
    "    new_column_name = f\"{from_column}_Normalized\"\n",
    "    print(f\"New column name: {new_column_name}\")\n",
    "    data[new_column_name] = data.apply(lambda linha: normalize_text(str(linha[from_column])), axis = 1)\n",
    "    return new_column_name\n",
    "\n",
    "\n",
    "normalized_summary_column_name = generate_normalized_column(data=df, from_column=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    decidir neste ano tarado vacina morte\n",
       "1        propósito doutor augusto aras vossa excelência...\n",
       "2        novo bolsonaro espalha mentiras vacinas desta ...\n",
       "3        gente óbvio bolsonaro tá vacinado óbvio esfreg...\n",
       "4        johnson perguntou bolsonaro tomado vacina nego...\n",
       "                               ...                        \n",
       "17296              folgamos saber dona olinda virou jacaré\n",
       "17297    jacaré parado vira bolsa bora lutar impeachmen...\n",
       "17298    acompanhei longo manhã cpi covid participação ...\n",
       "17299    cobaias desgoverno justo confusão aflição inse...\n",
       "17300    mãe feliz vida pq ontem grande vacinou virou j...\n",
       "Name: text_Normalized, Length: 17301, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_Normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adição de coluna de polaridade e subjetividade com base em outra coluna\n",
    "def generate_polarity_and_subjectivity_columns(data: pd.DataFrame, from_column: str):\n",
    "    new_polarity_column_name = f\"{from_column}_Polarity\"\n",
    "    data[new_polarity_column_name] = data[from_column].apply(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "    new_subjectivity_column_name = f\"{from_column}_Subjectivity\"\n",
    "    data[new_subjectivity_column_name] = data[from_column].apply(lambda text: TextBlob(text).sentiment.subjectivity)\n",
    "\n",
    "    return new_polarity_column_name, new_subjectivity_column_name\n",
    "\n",
    "normalized_summary_polarity_column_name, normalized_summary_subjectivity_column_name = generate_polarity_and_subjectivity_columns(data=df, from_column=normalized_summary_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação de lematização em coluna, retornando uma lista de palavras em sua forma raiz\n",
    "def apply_lemmatization_to_column(data: pd.DataFrame, column: str, allowed_postags: list[str] = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    lemma_column_words = []\n",
    "    for words in data[column]:\n",
    "        doc = nlp(\" \".join(words))\n",
    "        lemma_column_words.append(\" \".join([token.lemma_ for token in doc if token.pos_ in allowed_postags]))\n",
    "\n",
    "    return lemma_column_words\n",
    "#if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags\n",
    "df['lemma_column'] = apply_lemmatization_to_column(data=df, column=normalized_summary_column_name)\n",
    "df['text_lemma'] = apply_lemmatization_to_column(data=df, column='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17301 entries, 0 to 17300\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   Unnamed: 0                             17301 non-null  int64  \n",
      " 1   created_at                             17301 non-null  object \n",
      " 2   text                                   17301 non-null  object \n",
      " 3   public_metrics.like_count              17301 non-null  int64  \n",
      " 4   public_metrics.quote_count             17301 non-null  int64  \n",
      " 5   public_metrics.reply_count             17301 non-null  int64  \n",
      " 6   public_metrics.retweet_count           17301 non-null  int64  \n",
      " 7   author.id                              17301 non-null  int64  \n",
      " 8   author.created_at                      17301 non-null  object \n",
      " 9   author.description                     17289 non-null  object \n",
      " 10  author.location                        16649 non-null  object \n",
      " 11  author.public_metrics.followers_count  17301 non-null  int64  \n",
      " 12  author.public_metrics.following_count  17301 non-null  int64  \n",
      " 13  author.public_metrics.listed_count     17301 non-null  int64  \n",
      " 14  author.public_metrics.tweet_count      17301 non-null  int64  \n",
      " 15  nome                                   17301 non-null  object \n",
      " 16  nome_urna                              17301 non-null  object \n",
      " 17  partido                                17301 non-null  object \n",
      " 18  sigla_partido                          17301 non-null  object \n",
      " 19  PARTIDO_ATUAL                          17293 non-null  object \n",
      " 20  text_Normalized                        17301 non-null  object \n",
      " 21  text_Normalized_Polarity               17301 non-null  float64\n",
      " 22  text_Normalized_Subjectivity           17301 non-null  float64\n",
      " 23  lemma_column                           17301 non-null  object \n",
      " 24  text_lemma                             17301 non-null  object \n",
      "dtypes: float64(2), int64(10), object(13)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/processData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 10 clusters with 6 clusters populated\n",
      "In stage 1: transferred 12 clusters with 7 clusters populated\n",
      "In stage 2: transferred 9 clusters with 8 clusters populated\n",
      "In stage 3: transferred 11 clusters with 7 clusters populated\n",
      "In stage 4: transferred 7 clusters with 6 clusters populated\n",
      "In stage 5: transferred 10 clusters with 7 clusters populated\n",
      "In stage 6: transferred 9 clusters with 5 clusters populated\n",
      "In stage 7: transferred 10 clusters with 5 clusters populated\n",
      "In stage 8: transferred 10 clusters with 6 clusters populated\n",
      "In stage 9: transferred 11 clusters with 6 clusters populated\n",
      "In stage 10: transferred 10 clusters with 6 clusters populated\n",
      "In stage 11: transferred 11 clusters with 7 clusters populated\n",
      "In stage 12: transferred 10 clusters with 7 clusters populated\n",
      "In stage 13: transferred 11 clusters with 6 clusters populated\n",
      "In stage 14: transferred 11 clusters with 6 clusters populated\n",
      "In stage 15: transferred 12 clusters with 8 clusters populated\n",
      "In stage 16: transferred 11 clusters with 7 clusters populated\n",
      "In stage 17: transferred 9 clusters with 6 clusters populated\n",
      "In stage 18: transferred 9 clusters with 6 clusters populated\n",
      "In stage 19: transferred 7 clusters with 6 clusters populated\n",
      "In stage 20: transferred 10 clusters with 5 clusters populated\n",
      "In stage 21: transferred 9 clusters with 5 clusters populated\n",
      "In stage 22: transferred 10 clusters with 6 clusters populated\n",
      "In stage 23: transferred 10 clusters with 7 clusters populated\n",
      "In stage 24: transferred 11 clusters with 7 clusters populated\n",
      "In stage 25: transferred 9 clusters with 6 clusters populated\n",
      "In stage 26: transferred 10 clusters with 6 clusters populated\n",
      "In stage 27: transferred 10 clusters with 8 clusters populated\n",
      "In stage 28: transferred 9 clusters with 6 clusters populated\n",
      "In stage 29: transferred 9 clusters with 7 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = MovieGroupProcess(K=20, alpha=0.01, beta=0.01, n_iters=30)\n",
    "vocab = set(x for x in normalized_summary_column_name)\n",
    "vocab_size = len(vocab)\n",
    "output = mgp.fit(normalized_summary_column_name, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster,sort_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [0 1 1 4 3 0 2 0 0 0 0 0 0 2 0 0 0 2 0 0]\n",
      "\n",
      "Most important clusters (by number of docs inside): [ 3  4 17 13  6  1  2 19  8  5]\n",
      "\n",
      "Cluster 3 : [('o', 1), ('l', 1), ('z', 1), ('d', 1)]\n",
      "\n",
      "Cluster 4 : [('e', 2), ('_', 1)]\n",
      "\n",
      "Cluster 17 : [('t', 2)]\n",
      "\n",
      "Cluster 13 : [('r', 1), ('a', 1)]\n",
      "\n",
      "Cluster 6 : [('x', 1), ('i', 1)]\n",
      "\n",
      "Cluster 1 : [('N', 1)]\n",
      "\n",
      "Cluster 2 : [('m', 1)]\n",
      "\n",
      "Cluster 19 : []\n",
      "\n",
      "Cluster 8 : []\n",
      "\n",
      "Cluster 5 : []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('\\nMost important clusters (by number of docs inside):', top_index)\n",
    "# show the top 5 words in term frequency for each cluster \n",
    "top_words(mgp.cluster_word_distribution, top_index, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
